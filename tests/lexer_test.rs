#[cfg(test)]
mod tests {
    use rust_ts_json_compiler::lexer::{Lexer, Token};

    #[test]
    fn test_next_token() {
        let schema = "z.object({
  products: z
    .array(
      z.object({
        productId: z.number().int(),
        amount: z.number().multipleOf(1)
      })
    )
    .min(1),
  details: z.object({
    firstName: z.string(),
    lastName: z.string(),
    email: z.string().email(),
    phone: z.string()
  }),
  comment: z.string().optional(),
  cityId: z.number().int(),
  userId: z.string().uuid().optional()
})";
        let mut lex = Lexer::new(schema);
        let data = vec![
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("object".to_string()),
            Token::LRound,
            Token::LCurly,
            Token::Ident("products".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("array".to_string()),
            Token::LRound,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("object".to_string()),
            Token::LRound,
            Token::LCurly,
            Token::Ident("productId".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("number".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("int".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Comma,
            Token::Ident("amount".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("number".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("multipleOf".to_string()),
            Token::LRound,
            Token::Int("1".to_string()),
            Token::RRound,
            Token::RCurly,
            Token::RRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("min".to_string()),
            Token::LRound,
            Token::Int("1".to_string()),
            Token::RRound,
            Token::Comma,
            Token::Ident("details".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("object".to_string()),
            Token::LRound,
            Token::LCurly,
            Token::Ident("firstName".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("string".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Comma,
            Token::Ident("lastName".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("string".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Comma,
            Token::Ident("email".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("string".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("email".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Comma,
            Token::Ident("phone".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("string".to_string()),
            Token::LRound,
            Token::RRound,
            Token::RCurly,
            Token::RRound,
            Token::Comma,
            Token::Ident("comment".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("string".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("optional".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Comma,
            Token::Ident("cityId".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("number".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("int".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Comma,
            Token::Ident("userId".to_string()),
            Token::Colon,
            Token::Ident("z".to_string()),
            Token::Dot,
            Token::Ident("string".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("uuid".to_string()),
            Token::LRound,
            Token::RRound,
            Token::Dot,
            Token::Ident("optional".to_string()),
            Token::LRound,
            Token::RRound,
            Token::RCurly,
            Token::RRound,
            Token::Eof,
        ];

        for (i, t) in data.into_iter().enumerate() {
            let token = lex.next();
            assert_eq!(token, t, "test {i} expected={t:?}, got={token:?}");
        }
    }
}
